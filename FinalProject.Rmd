---
title: "Final Project - "
author: "Aaron Abromowitz, Stephanie Duarte, Dammy Owolabi"
date: "2024-04-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# EDA
The first thing that is done when you get a dataset is to peform an Exploratory Data Anslysis, to see what characteristics the data has.  The variable we are trying to predict is the y column whic represents if the client has subscribed to a term deposit.  The possible values are "yes" or "no".

## Create training and test set
Going forward, we will use the training set for all analysis and model building.  The test set will be used at the end to get metrics for the various models we create.
```{r}
# Pull in data
data<-read.csv('https://raw.githubusercontent.com/stedua22/6372-Project-2/main/bank-additional-full.csv',stringsAsFactors = T, sep=";")

# Set levels to use for later
data$y <- relevel(data$y, ref="yes")
data$month <- factor(data$month, levels=c('mar','apr','may','jun','jul','aug','sep','oct','nov','dec'))
data$day_of_week <- factor(data$day_of_week, levels=c('mon','tue','wed','thu','fri'))

# Create the train and test split
train_perc <- .8
set.seed(1234)
train_indices <- sample(nrow(data), floor(train_perc * nrow(data)))
train_data <- data[train_indices, ]
nrow(train_data)
test_data <- data[-train_indices, ] 
nrow(test_data)
train_data$duration <- c()
```

## Look at summary statistics for numeric variables
There are several numeric variables where we can look at the min/max, quartiles, median, and mean.
```{r}
summary(train_data$age)
summary(train_data$campaign)
summary(train_data$pdays)
summary(train_data$previous)
summary(train_data$emp.var.rate)
summary(train_data$cons.price.idx)
summary(train_data$cons.conf.idx)
summary(train_data$euribor3m)
summary(train_data$nr.employed)
```

## Look at summary statistics for categorical variables
There are several categorical variables.  Summary statistics don't make as much sense for them, but you can look at the distribution of values in the different categories.
```{r}
summary(train_data$job)
summary(summary(train_data$job))
length(summary(train_data$job))

summary(train_data$marital)
summary(summary(train_data$marital))
length(summary(train_data$marital))

summary(train_data$education)
summary(summary(train_data$education))
length(summary(train_data$education))

summary(train_data$default)
summary(summary(train_data$default))
length(summary(train_data$default))

summary(train_data$housing)
summary(summary(train_data$housing))
length(summary(train_data$housing))

summary(train_data$loan)
summary(summary(train_data$loan))
length(summary(train_data$loan))

summary(train_data$contact)
summary(summary(train_data$contact))
length(summary(train_data$contact))

summary(train_data$month)
summary(summary(train_data$month))
length(summary(train_data$month))

summary(train_data$day_of_week)
summary(summary(train_data$day_of_week))
length(summary(train_data$day_of_week))

summary(train_data$poutcome)
summary(summary(train_data$poutcome))
length(summary(train_data$poutcome))
```

## Examine bank client data
The dataset includes age, job, marital, education, default, housing, and loan columns, which were identified as client data.

```{r}
library(tidyverse)

# Plot age
summary <- train_data %>%
  group_by(age,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=age,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Age') + ggtitle('Age Distribution Based on Y Value')

# Plot job
summary <- train_data %>%
  group_by(job,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=job,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Job') + ggtitle('Job Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot marital
summary <- train_data %>%
  group_by(marital,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=marital,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Marital') + ggtitle('Marital Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot default
summary <- train_data %>%
  group_by(default,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=default,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Default') + ggtitle('Default Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot housing
summary <- train_data %>%
  group_by(housing,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=housing,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Housing') + ggtitle('Housing Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
<br/>
None of these variables appear to show any strong indicator for yes or no.  It looks like higher ages tend to lean more towards yes, and certain jobs (Ex: admin) lean more towards yes.  When Y = no, there are more about double the unknown values, but still far under 50%.

## Examine data related with the last contact of the current campaign
The dataset includes contact communication type, month of contact, and day of week of contac, for the last contact of the current campaign to sell term deposits.
```{r}
# Plot contact
summary <- train_data %>%
  group_by(contact,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=contact,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Contact') + ggtitle('Contact Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot month
summary <- train_data %>%
  group_by(month,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=month,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Month') + ggtitle('Month Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot day of week
summary <- train_data %>%
  group_by(day_of_week,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=day_of_week,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Day of Week') + ggtitle('Day of Week Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
<br/>
If y is Yes, there are ~20% more likely to be contacted on your cell phone than on landline.  There are certain months that also seem to have more term deposit sales in them.  Day of week looks to not have much change.

## Examine other data related with the current campaign or previous campaigns
There are variables for number of contacts performed during this campaign and for this client (campaign), number of days that passed by after the client was last contacted from a previous campaign (pdays), number of contacts performed before this campaign and for this client (previous), and outcome of the previous marketing campaign (poutcome).
```{r}
# Plot campaign
summary <- train_data %>%
  group_by(campaign,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=campaign,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Campaign') + ggtitle('Campaign Distribution Based on Y Value') + xlim(c(0,20))

# Plot poutcome
summary <- train_data %>%
  group_by(poutcome,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=poutcome,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Poutcome') + ggtitle('Poutcome Percentages Based on Y Value') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
<br/>
Campaign distributions seem pretty similar between Yes and No.  For Poutcome, success is more common for the yes than for no.

## Examine socio-economic data
There are variables for socio economic data for Employement Variation Rate, Consumer Price Index, Consumer Confidence Index, Euribor 3 month rate, and Numer of Employees.
```{r}
# Plot emp.var.rate
summary <- train_data %>%
  group_by(emp.var.rate,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=emp.var.rate,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Employment Variation Rate') + ggtitle('Employment Variation Rate Distribution Based on Y Value')

# Plot cons.price.idx
summary <- train_data %>%
  group_by(cons.price.idx,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=cons.price.idx,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Consumer Price Index') + ggtitle('Consumer Price Index Distribution Based on Y Value')

# Plot euribor3m
summary <- train_data %>%
  group_by(euribor3m,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=euribor3m,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Euribor3m') + ggtitle('Euribor3m Metric Distribution Based on Y Value')

# Plot nr.employed
summary <- train_data %>%
  group_by(nr.employed,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=nr.employed,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Number of Employees') + ggtitle('Number of Employees Distribution Based on Y Value')

### Analyzing campaign

ggplot(train_data) +   
  geom_histogram(mapping = aes(x=campaign, fill=y)) +  
  ggtitle("Distribution of 'y' by campaign")

### Analyzing JOB

ggplot(train_data) +   
  geom_bar(mapping = aes(x=job, fill = y)) +   
  coord_flip() +     #Added coord flip here to make it more readable
  ggtitle("Number of 'y' by job") +  
  ylab("Count") +   
  xlab("Job")
```
<br/>
Admin, technician and blue collar jobs are the top 3 subscribers by volume 

```{r}
df2 <- train_data %>%  
  group_by(job) %>%  
  count(y) %>%  
  mutate(job_conv = n/sum(n)) %>%  
  filter(y == "yes")

ggplot(df2, aes(x=job, y=job_conv)) +  
  geom_point() +  
  coord_flip() 
```

Above, I looked at the ratio of "yes" vs "no" and see that students and retired persons convert at much higher rates than those of other professions. And 'blue collar' has the lowest conversion rate

So, if they were to want to improve the cost effectiveness of their campaigns they might want to target more 'students' and 'retirees'


### Analyzing By Month
```{r}
ggplot(train_data) + 
  geom_bar(mapping = aes(x=month, fill = y)) + 
  ggtitle("Number of 'y' by month") +
  ylab("Cnt") + xlab("month")

#Education
ggplot(train_data, aes(x = education, fill = y)) + 
  geom_bar(position = "fill") + 
  ggtitle("Distribution of 'y' by Education") +  
  ylab("Proportion") + 
  xlab("Education Level")

ggplot(train_data, aes(x = education, fill = y)) + 
  geom_bar() + 
  facet_wrap(~ y, scales = "free_y") + 
  ggtitle("Distribution of 'y' by Education") +  
  ylab("Count") + 
  xlab("Education Level")

  
#Age
ggplot(train_data) + 
  geom_bar(mapping = aes(x=age, fill = y)) + 
  ggtitle("Distribution of 'y' by Age Group") +  
  ylab("Cnt") + 
  xlab("Age Group")

# by nr.employed
ggplot(train_data) + geom_histogram(mapping = aes(x = nr.employed, fill = y)) +
  ggtitle("Distribution of 'y' by nr.employed")

# Euribor 3 month rate
ggplot(train_data, aes(x = month , y = euribor3m, fill = y)) +  
  geom_boxplot() +   
  ggtitle("euribor3m by month")

ggplot(train_data) + geom_histogram(mapping = aes(x = euribor3m, fill = y)) +
  ggtitle("Distribution of euribor3m by month")

# Correlations of socio-economic variables
library(GGally)
ggpairs(train_data[,c('emp.var.rate','cons.price.idx','nr.employed','euribor3m')],aes(color = train_data$y))
```
<br/>
The Consumer Price Index data seems to have more of a flat distribution for those with a term deposit.  Employment Variation Rate, Euribor3m, and Number Employed all seem to have similar distributions where there is a higher percentage without term deposits for higher values of the index. <br/>
<br/>

Looking at the paired correlations between out of the socio economic variables, we can see that Employment Variation Rate, Euribor3m, and Number Employed are highly correlated.

## Clustering
We wanted to see if a clustering analysis of the data would be useful.  We only looked at the numeric variables.  The purpose of a clustering analysis is to see if splitting the data into clusters allows for additional insight, particularly into classification. <br/>
<br/>

First, we will try to determine the number of clusters to use.  The metric we used for comparing cluster sizes is the Silhoutte Statistic.  Below is the code I ran, but it has trouble knitting.  The heat maps are in the PPT though.
```{r, eval=FALSE}
library(RColorBrewer)
library(pheatmap)
library(cluster)
df.numeric <- train_data[ , sapply(train_data, is.numeric)]
center.scale=scale(df.numeric)
mydist<-dist(center.scale)
sim.clust<-hclust(mydist,method="complete")
max_clusters <- 20
my.sil<-c()
for (i in 2:max_clusters){
  print(i)
  sil.result<-silhouette(cutree(sim.clust,i),mydist)
  my.sil[i-1]<-summary(sil.result)$avg.width
}
```

```{r}
max_clusters <- 20
my.sil <- read.csv('https://raw.githubusercontent.com/stedua22/6372-Project-2/main/my_sil.csv')
my.sil <- my.sil$x
ggplot(data = data.frame(x=2:max_clusters, y=my.sil),aes(x=x,y=y)) + geom_line() + 
  ylab('Silhouette') + xlab('Cluster Size') + ggtitle('Determining Number of Clusters to Use') + 
  geom_point(data = data.frame(x = 2, y = my.sil[1]), aes(x = x, y = y), size = 1, color = "red", fill = "red", shape = 21)
```
<br/>
It looks like 2 clusters is the highest, so we will try that.

```{r, eval=FALSE}
num_clusters <- 2
rownames(df.numeric)<-paste("R",1:nrow(df.numeric),sep="")
annotation_row<-data.frame(Response=factor(train_data$y),Cluster=factor(cutree(sim.clust,num_clusters)))
rownames(annotation_row)<-rownames(df.numeric)
pheatmap(df.numeric,annotation_row=annotation_row,cluster_cols=F,scale="column",fontsize_row=3,legend=T
         ,color=colorRampPalette(c("blue","white", "red"), space = "rgb")(100))
```
![Heatmap with 2 Clusters](https://raw.githubusercontent.com/stedua22/6372-Project-2/main/Heatmap%202%20Clusters.png)
<br/>
One of the clusters is huge and the other is tiny.  Let's try with 11 clusters.

```{r, eval=FALSE}
num_clusters <- 11
rownames(df.numeric)<-paste("R",1:nrow(df.numeric),sep="")
annotation_row<-data.frame(Response=factor(train_data$y),Cluster=factor(cutree(sim.clust,num_clusters)))
rownames(annotation_row)<-rownames(df.numeric)
pheatmap(df.numeric,annotation_row=annotation_row,cluster_cols=F,scale="column",fontsize_row=3,legend=T
         ,color=colorRampPalette(c("blue","white", "red"), space = "rgb")(100))
```
![Heatmap with 11 Clusters](https://raw.githubusercontent.com/stedua22/6372-Project-2/main/Heatmap%2011%20Clusters.png)
<br/>
This is a little bit more interesting.  Some clusters had a lot of 'yes' results.  Others seem to be the same distribution as before.

## Variables over time
We want to see if there was any variation in term deposits over time.  The file explaining the dataset mentioned that the data was in order of date.
```{r}
# Sorting data by row number
train_data_sorted <- train_data
train_data_sorted$num <- as.numeric(rownames(train_data_sorted))
train_data_sorted$group <- ceiling(train_data_sorted$num / 1000)
summary <- train_data_sorted %>%
  group_by(group,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data_sorted[train_data_sorted$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data_sorted[train_data_sorted$y == 'yes',]) * 100

# Term deposit over time
summary %>% ggplot(aes(x=group,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Term Deposits') + xlab('Time') + ggtitle('Term Deposits over Time')

# Month over time
train_data_sorted %>% ggplot(aes(x=num, y=month, color=y)) + geom_jitter() + 
  xlab('Time') + ylab('Month') + ggtitle('Last Contact Month over Time')

# Employment Variation Rate over time
train_data_sorted %>% ggplot(aes(x=num, y=emp.var.rate, color=y)) + geom_jitter() + 
  xlab('Time') + ylab('Month') + ggtitle('Employment Variation Rate over Time')
```
<br/>
There definitely is a change in the data over time.  We may re-visit this later on.

# Objective 1: Simple Logistic Regerssion Model
We used a combination of forward and backward selection to determine a simple logistic regression model that performed well on the data.  We did this by adding each variable to a model, and then doing Cross Validation to test the out of sample data on AUROC (Area under the ROC curve).  We used 10 folds. <br/>
<br/>
After we chose the first variable, we then did this to add more variables to see if those increased the AUROC.  We also tried removing variables (once we got three of them) to see if that improved the score.  Below, I included example code for this logic.  It takes several minutes to run though, so it is not set to evaluate the code. <br/>
<br/>
One caveat is that since the Default variable only has 2 Yes values, it can cause errors sometimes during the cross validation.  This happens when the training data (a randomly chosen 90%) doesn't have either of those values, and the test data (the other 10%) has both of them.  This only happens 1% of the time, but when you are running 100s of tests, this happens regularly.  And since Default didn't increase the AUROC much anyways, we decided to drop the variable.
```{r}
train_data$default <- c()
```

```{r, eval = FALSE}
# Forward Selection Example
set.seed(21)
vars <- names(train_data)
vars <- vars[vars!="y"]
num_vars <- length(vars)
var_aucs <- data.frame("vars" = vars)
num_folds <- 10
for (j in 1:num_vars) {
  var <- vars[j]
  print(var)
  folds <- createFolds(train_data$y, k = num_folds)
  auc_scores <- numeric(num_folds)
  for (i in 1:num_folds) {
    train_indices <- unlist(folds[-i])
    test_indices <- unlist(folds[i])
    train <- train_data[train_indices, ]
    test <- train_data[test_indices, ]
    form <- as.formula(paste("y ~ ",var,sep=""))
    model <- glm(form, data = train, family = "binomial")
    predictions <- predict(model, newdata = test, type = "response")
    roc <- roc(response=test$y,predictor=predictions,levels=c("no", "yes"),direction = ">")
    auc_scores[i] <- auc(roc)
  }
  var_aucs$auc[var_aucs$var == var] <- mean(auc_scores)
}

# Backward selection example
set.seed(24)
start_form_str <- 'y ~ nr.employed + month + poutcome'
vars <- c('nr.employed','month','poutcome')
num_vars <- length(vars)
var_aucs <- data.frame("vars" = vars)
num_folds <- 10
for (j in 1:num_vars) {
  var <- vars[j]
  print(var)
  folds <- createFolds(train_data$y, k = num_folds)
  auc_scores <- numeric(num_folds)
  for (i in 1:num_folds) {
    train_indices <- unlist(folds[-i])
    test_indices <- unlist(folds[i])
    train <- train_data[train_indices, ]
    test <- train_data[test_indices, ]
    form <- as.formula(paste(start_form_str," -",var,sep=""))
    model <- glm(form, data = train, family = "binomial")
    predictions <- predict(model, newdata = test, type = "response")
    roc <- roc(response=test$y,predictor=predictions,levels=c("no", "yes"),direction = ">")
    auc_scores[i] <- auc(roc)
  }
  var_aucs$auc[var_aucs$var == var] <- mean(auc_scores)
}
```
<br/>
After we did this until the AUROC didn't increase any more, the resulting model was y ~ month + poutcome + emp.var.rate + euribor3m + contact + cons.price.idx.  We then checked the p values and VIR to see if it made sense to keep all of those variables.

```{r}
library(car)
model <- glm(y ~ month + poutcome + emp.var.rate + euribor3m + contact + cons.price.idx, data = train_data, family = "binomial")
summary(model)
vif(model)
```
<br/>
The p values were all significant at the 0.05 level, but there was a high amount of correlation between emp.var.rate and euribor3m.  So we removed euribor3m to see if that didn't make the model too much worse.

```{r}
library(caret)
library(pROC)
set.seed(80)
form <- as.formula('y ~ month + poutcome + emp.var.rate + contact + cons.price.idx')
num_folds <- 10
folds <- createFolds(train_data$y, k = num_folds)
accuracy_scores <- numeric(num_folds)
auc_scores <- numeric(num_folds)
for (i in 1:num_folds) {
  train_indices <- unlist(folds[-i])
  test_indices <- unlist(folds[i])
  train <- train_data[train_indices, ]
  test <- train_data[test_indices, ]
  model <- glm(form, data = train, family = "binomial")
  predictions <- predict(model, newdata = test, type = "response")
  roc <- roc(response=test$y,predictor=predictions,levels=c("no", "yes"),direction = ">")
  auc_scores[i] <- auc(roc)
}
mean(auc_scores)  
```
<br/>
It wasn't too much worse.  And removing the correlation between those two variables makes the model easier to interpret.

```{r}
model <- glm(y ~ month + poutcome + emp.var.rate + contact + cons.price.idx, data = train_data, family = "binomial")
summary(model)
vif(model)
```
<br/>
Now the VIFs are much more resonable without euribor3m.  So we chose the simpler model for our Simple Logistic Regression Model.

# Objective 2: Complex Logistic Regerssion Model
For the second model, we looked into adding polynomial terms and/or interaction terms to the regression model.

## Polynomial Terms
To see if it made sense to add some polynomial terms we looked at what happened with adding those for Number of Employees, since that was the first variable added during Forward Selection (although it got removed later).
```{r}
# Make a nr.employed^2 variable
train_data$ne2 = (train_data$nr.employed)^2

# Plot nr.employed
summary <- train_data %>%
  group_by(nr.employed,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=nr.employed,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Number Employed') + ggtitle('Number Employed Based on Y Value')

# Plot nr.employed^2
summary <- train_data %>%
  group_by(ne2,y) %>%
  summarize(count=n())
summary$perc <- 0
summary$perc[summary$y == 'no'] <- summary$count[summary$y == 'no'] / nrow(train_data[train_data$y == 'no',]) * 100
summary$perc[summary$y == 'yes'] <- summary$count[summary$y == 'yes'] / nrow(train_data[train_data$y == 'yes',]) * 100
summary %>% ggplot(aes(x=ne2,y=perc,fill=y)) + geom_bar(stat="identity") + facet_wrap(~y) + 
  ylab('Percentage') + xlab('Number Employed') + ggtitle('Number Employed Squared Based on Y Value')
```
<br/>
These plots look very hard to distinguish.  So instead of plotting polynomial terms, we tried creating simple models and then calculating out of sample AUC.  If this increases as polynomial degree increases, then it could make sense to include polynomial terms.

```{r}
# Maybe just plot the improvement of out of sample AUC
set.seed(120)
vars <- 'nr.employed'
allVars <- vars
num_poly <- 10
for (i in 1:length(vars)){
  for (j in 2:num_poly){
    if (class(train_data[,vars[i]]) != "factor") {
      allVars <- c(allVars,paste('poly(',vars[i],',',j,')',sep=""))
    }
  }
}
num_vars <- length(allVars)
var_aucs <- data.frame("vars" = allVars)
num_folds <- 10
for (j in 1:num_vars) {
  var <- allVars[j]
  print(var)
  folds <- createFolds(train_data$y, k = num_folds)
  auc_scores <- numeric(num_folds)
  for (i in 1:num_folds) {
    train_indices <- unlist(folds[-i])
    test_indices <- unlist(folds[i])
    train <- train_data[train_indices, ]
    test <- train_data[test_indices, ]
    form <- as.formula(paste("y ~ ",var,sep=""))
    model <- glm(form, data = train, family = "binomial")
    predictions <- predict(model, newdata = test, type = "response")
    roc <- roc(response=test$y,predictor=predictions,levels=c("no", "yes"),direction = ">")
    auc_scores[i] <- auc(roc)
  }
  var_aucs$auc[var_aucs$var == var] <- mean(auc_scores)
}

# Get max val
maxAUC <- max(var_aucs$auc, na.rm = TRUE)
maxDeg <- which.max(var_aucs$auc)

# Looking at the p values for the highest degree model
model <- glm(form, data = train_data, family = "binomial")
summary(model) 

# Plot the AUC improving
var_aucs %>% ggplot(aes(x=1:10, y=auc)) + geom_point() + ylim(c(0.7,0.8)) + 
  ylab('AUC') + xlab('Polynomial Degree') + ggtitle('Out of Sample AUC for nr.employed') + 
  geom_point(data = data.frame(x = maxDeg, y = maxAUC), 
             aes(x = x, y = y), size = 2, color = "red", fill = "red", shape = 21)
```
You can see from the plot that there is a pretty substantial increase in performance after 4 polynomial terms.  It levels off after that, but the maximum AUC is technically at n = 9.  And even the model for n = 10 shows that many of the higher degree terms are statistically significant, including the 10th term.<br/>
<br/>

We will proceed with trying forward selection using polynomial terms.  Using up to the 10th degree term seems like it should be sufficient.

## Variable Interactions
Variable interactions are often present that affect numeric response variables.  Usually to show that, you can plot.  To see if there are possible interactions, let's try with a coloring a couple of interactions.

```{r}
# Try plotting month by contact
train_data %>% ggplot(aes(x=month,y=contact,color=y)) + geom_jitter() + 
  ylab('Month') + xlab('Contact') + ggtitle('Term Deposit for Month and Contact')

# Looking at model
model <- glm('y ~ month*contact', data = train_data, family = "binomial")
summary(model)

# Try plotting month by day of week
train_data %>% ggplot(aes(x=month,y=day_of_week,color=y)) + geom_jitter() + 
  ylab('Month') + xlab('Day of Week') + ggtitle('Term Deposit for Month and Day of Week')

# Looking at model
model <- glm('y ~ month*day_of_week', data = train_data, family = "binomial")
summary(model)
```
It looks like there is some promise for variable interactions.  When looking at Month vs Day of Week, certain months have days that have a different distribution of yes and no.  And certain days have months that have a different distribution.  Some of the interaction terms also have significant p values. <br/>
We will also include interaction terms in the Forward Selection, along with the polynomial terms.  Below is some example code (not being evaluate, because it takes over an hour to finish) of adding all the single variables, polynomial variables, and interaction terms to the model.
```{r, eval = FALSE}
set.seed(70)
vars <- colnames(train_data)
vars <- vars[vars!="y"]
allVars <- vars
num_poly <- 10
for (i in 1:length(vars)){
  for (j in 2:num_poly){
    if (class(train_data[,vars[i]]) != "factor") {
      allVars <- c(allVars,paste('poly(',vars[i],',',j,')',sep=""))
    }
  }
}
for (i in 1:length(vars)){
  for (j in 1:i){
    if(vars[i]!=vars[j]) {
      allVars <- c(allVars,paste(vars[i],'*',vars[j],sep=''))
    }
  }
}
# These variables need to be removed so the code doesn't error out
allVars <- allVars[allVars!="poly(pdays,6)"] 
allVars <- allVars[allVars!="poly(pdays,7)"]
allVars <- allVars[allVars!="poly(pdays,8)"]
allVars <- allVars[allVars!="poly(pdays,9)"]
allVars <- allVars[allVars!="poly(pdays,10)"]
allVars <- allVars[allVars!="poly(previous,7)"]
allVars <- allVars[allVars!="poly(previous,8)"]
allVars <- allVars[allVars!="poly(previous,9)"]
allVars <- allVars[allVars!="poly(previous,10)"]
allVars <- allVars[allVars!="poly(emp.var.rate,10)"]
var_aucs <- data.frame("vars" = allVars)
num_vars <- length(allVars)
num_folds <- 10
start_num <- 124
for (j in start_num:num_vars) {
  var <- allVars[j]
  print(paste(j,'/',num_vars,': ',var,sep=''))
  folds <- createFolds(train_data$y, k = num_folds)
  auc_scores <- numeric(num_folds)
  for (i in 1:num_folds) {
    train_indices <- unlist(folds[-i])
    test_indices <- unlist(folds[i])
    train <- train_data[train_indices, ]
    test <- train_data[test_indices, ]
    form <- as.formula(paste("y ~ ",var,sep=""))
    model <- glm(form, data = train, family = "binomial")
    predictions <- predict(model, newdata = test, type = "response")
    roc <- roc(response=test$y,predictor=predictions,levels=c("no", "yes"),direction = ">")
    auc_scores[i] <- auc(roc)
  }
  var_aucs$auc[var_aucs$var == var] <- mean(auc_scores)
}
```
After several iterations of this (plus Backwards Selection), we arrived at the final model: y ~ poly(cons.conf.idx,10) + pdays + day_of_week * month + month * contact + cons.conf.idx * housing + poutcome * previous + poly(campaign,5) + poly(euribor3m,8)  + campaign * month + cons.conf.idx * age + poly(previous,6) + campaign * contact + poly(age,3).

# Calculating Metrics
We are calculating Sensitivity (Correctly Predicted Positive / All Positive), Specificity (Correctly Predicted Negative / All Negative), Prevalence (All Positive / All Observations), PPV (Correctly Predicted Positive / Predicted Positive), NPV (Correctly Predicted Negative / Predicted Negative), and AUROC (Area under the ROC Curve). <br/>
<br/>

In addition, we are also calculating the F1 score.  This is equal to 2 * Sensitivity * PPV / (Sensitivity + PPV).  Since it is more important that we do a good job of predicting whether people will get a term deposit, and the F1 score is a nice metric to make sure that there is a good balance between Sensitivity and PPV, we decided to track this metric as well.

## Simple Logisitc Regression Model
First we determined the threshold to use for the Simple Logistic Regression Model in order to maximize the F1 metric.

```{r}
# Get threshold
metrics = data.frame(thresh=seq(0, 1, by = 0.0001))
num_thresh <- nrow(metrics)
metrics$sensitivity <- 0
metrics$specificity <- 0
metrics$ppv <- 0
metrics$npv <- 0
metrics$accuracy <- 0
metrics$f1 <- 0
form <- as.formula(y ~ month + poutcome + emp.var.rate + contact + cons.price.idx)
model <- glm(form, data = train_data, family = "binomial")
predicted <- predict(model, newdata = train_data, type = "response")
```

```{r, eval = FALSE}
for (i in 1:num_thresh){
  
  # Confusion Matrix
  predicted_classes <- ifelse(predicted > metrics$thresh[i], 'no','yes')
  confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(train_data$y))
  
  # Metrics
  metrics$sensitivity[i] <- as.numeric(confusion_matrix$byClass['Sensitivity'])
  metrics$specificity[i] <- as.numeric(confusion_matrix$byClass['Specificity'])
  metrics$ppv[i] <- as.numeric(confusion_matrix$byClass['Pos Pred Value'])
  metrics$npv[i] <- as.numeric(confusion_matrix$byClass['Neg Pred Value'])
  metrics$accuracy[i] <- as.numeric(confusion_matrix$overall['Accuracy'])
  metrics$f1[i] <- as.numeric(confusion_matrix$byClass['F1'])
}
```
```{r}
# Get threshold value that maximizes F1
metrics <- read.csv('https://raw.githubusercontent.com/stedua22/6372-Project-2/main/metrics_simple.csv')
maxF1 <- max(metrics$f1, na.rm = TRUE)
maxF1
theshF1 <- metrics$thresh[which.max(metrics$f1)] 
theshF1

# Plots
metrics %>% ggplot(aes(x = thresh, y = sensitivity)) + geom_point() + 
  ylab('Sensitivity') + xlab('Thresholds') + ggtitle('Sensitivity for Training Data')
metrics %>% ggplot(aes(x = thresh, y = specificity)) + geom_point() + 
  ylab('Specificity') + xlab('Thresholds') + ggtitle('Specificity for Training Data')
metrics %>% ggplot(aes(x = thresh, y = ppv)) + geom_point() + 
  ylab('PPV') + xlab('Thresholds') + ggtitle('PPV for Training Data')
metrics %>% ggplot(aes(x = thresh, y = npv)) + geom_point() + 
  ylab('NPV') + xlab('Thresholds') + ggtitle('NPV for Training Data')
metrics %>% ggplot(aes(x = thresh, y = f1)) + geom_point() + 
  ylab('F1 Score') + xlab('Thresholds') + ggtitle('F1 Scores for Training Data') + 
  geom_point(data = data.frame(x = theshF1, y = maxF1), aes(x = x, y = y), size = 3, color = "red", fill = "red", shape = 21)

# Get Confusion Matrix for threshold value
predicted_classes <- ifelse(predicted > theshF1, 'no','yes')
confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(train_data$y))
confusion_matrix
confusion_matrix$byClass['F1']
```
<br/>
After we got the thresholds to use, then we calculated the metrics on the test dataset.

```{r}
# Test data
predicted <- predict(model, newdata = test_data, type = "response")
predicted_classes <- ifelse(predicted > theshF1, 'no','yes')
confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(test_data$y))
confusion_matrix
confusion_matrix$byClass['F1']

# AUC
roc <- roc(response=test_data$y,predictor=predicted,levels=c("no", "yes"),direction = ">")
auc(roc)
plot(roc,print.thres="best",col="red")
```
<br/>
This code takes a while to run, so we won't include the code to maximize the threshold value going forward.  Given the thresholds though, here is the code to get the metrics for the complicated logistic regression model.

```{r}
# Get threshold value that maximizes F1
metrics <- read.csv('https://raw.githubusercontent.com/stedua22/6372-Project-2/main/metrics_complex_logistic.csv')
maxF1 <- max(metrics$f1, na.rm = TRUE)
maxF1
theshF1 <- metrics$thresh[which.max(metrics$f1)] 
theshF1

# Get Confusion Matrix for threshold value
form <- as.formula(y ~ poly(cons.conf.idx,10) + pdays + day_of_week*month + month*contact + cons.conf.idx*housing + poutcome*previous + poly(campaign,5) + poly(euribor3m,8)  + campaign*month + cons.conf.idx*age + poly(previous,6) + campaign*contact + poly(age,3))
model <- glm(form, data = train_data, family = "binomial")
predicted <- predict(model, newdata = train_data, type = "response")
predicted_classes <- ifelse(predicted > theshF1, 'no','yes')
confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(train_data$y))
confusion_matrix
confusion_matrix$byClass['F1']

# Test data
predicted <- predict(model, newdata = test_data, type = "response")
predicted_classes <- ifelse(predicted > theshF1, 'no','yes')
confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(test_data$y))
confusion_matrix
confusion_matrix$byClass['F1']

# AUC
roc <- roc(response=test_data$y,predictor=predicted,levels=c("no", "yes"),direction = ">")
auc(roc)
plot(roc,print.thres="best",col="red")
```
