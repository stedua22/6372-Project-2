{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stedua22/6372-Project-2/blob/main/Copy_of_ICA2_DataMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ9wGDdTZ1bl"
      },
      "source": [
        "\n",
        "**NAME:**\n",
        "(enter your name here)\n",
        "\n",
        "\n",
        "# In Class Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65tpU9sfZ1bp"
      },
      "source": [
        "### Loading the Classification Data\n",
        "Please run the following code to read in the \"digits\" dataset from sklearn's data loading module. This is identical to the first in class assignment for loading the data into matrices. `ds.data` is a matrix of feature values and `ds.target` is a column vector of the class output (in our case, the hand written digit we want to classify). Each class is a number (0 through 9) that we want to classify as one of ten hand written digits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8uAw8HpZZ1bq",
        "outputId": "471c18ad-b33b-4fe6-9522-3fc35a8dedaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features shape: (1797, 64)\n",
            "target shape: (1797,)\n",
            "range of target: 0 9\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "%matplotlib inline\n",
        "\n",
        "ds = load_digits()\n",
        "\n",
        "# this holds the continuous feature data\n",
        "print('features shape:', ds.data.shape) # there are 1797 instances and 64 features per instance\n",
        "print('target shape:', ds.target.shape )\n",
        "print('range of target:', np.min(ds.target),np.max(ds.target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgCqnZEYZ1br"
      },
      "source": [
        "### Using Decision Trees\n",
        "In the videos, we talked about the splitting conditions for different attributes. Specifically, we discussed the number of ways in which it is possible to split a node, depending on the attribute types. To understand the possible splits, we need to understand the attributes. For the question below, you might find the description in the `ds['DESCR']` field to be useful. You can see the field using `print(ds['DESCR'])`\n",
        "\n",
        "**Question 1:** For the digits dataset, what are the type(s) of the attributes? How many attributes are there? What do they represent?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbnBZ2_lZ1bs"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The attributes are numerical, specifically integer values ranging from 0 to 9. There are 64 attribute. The attributes represent pixel intensity values of 8x8 images of handwritten digits. Each attribute corresponds to a specific pixel in the 8x8 grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Km0Ocs8UZ1bs",
        "outputId": "a7881c4b-bfcb-4557-b200-b093bac29b8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _digits_dataset:\n",
            "\n",
            "Optical recognition of handwritten digits dataset\n",
            "--------------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 1797\n",
            "    :Number of Attributes: 64\n",
            "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
            "    :Missing Attribute Values: None\n",
            "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
            "    :Date: July; 1998\n",
            "\n",
            "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
            "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
            "\n",
            "The data set contains images of hand-written digits: 10 classes where\n",
            "each class refers to a digit.\n",
            "\n",
            "Preprocessing programs made available by NIST were used to extract\n",
            "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
            "total of 43 people, 30 contributed to the training set and different 13\n",
            "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
            "4x4 and the number of on pixels are counted in each block. This generates\n",
            "an input matrix of 8x8 where each element is an integer in the range\n",
            "0..16. This reduces dimensionality and gives invariance to small\n",
            "distortions.\n",
            "\n",
            "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
            "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
            "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
            "1994.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
            "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
            "    Graduate Studies in Science and Engineering, Bogazici University.\n",
            "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
            "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
            "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
            "    Electrical and Electronic Engineering Nanyang Technological University.\n",
            "    2005.\n",
            "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
            "    Algorithm. NIPS. 2000.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Enter any code here\n",
        "print(ds['DESCR'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N_0cfbqgZ1bt",
        "outputId": "e0698fbc-f5b1-4431-f6b7-56bb4f3e8d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0.98, 'Label is 6')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#get the vector data from a random position\n",
        "idx = np.random.randint(0,len(ds.target))\n",
        "raw_row = ds.data[idx] # as a vector\n",
        "img = ds.data[idx].reshape((8,8)) #reshaped to be an image\n",
        "\n",
        "#custom plotting tools\n",
        "gs = gridspec.GridSpec(4,2)\n",
        "\n",
        "plt.subplot(gs[0,:]) #show vector\n",
        "plt.imshow(np.matrix(raw_row), cmap=plt.cm.bone, interpolation='nearest')\n",
        "plt.title('Row vector')\n",
        "\n",
        "plt.subplot(gs[1:,:]) #show image\n",
        "plt.imshow(img, cmap=plt.cm.bone, interpolation='nearest')\n",
        "plt.title('Vector reshaped as an 8x8 image')\n",
        "\n",
        "plt.suptitle('Label is %d'%(ds.target[idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PceESQaZ1bt"
      },
      "source": [
        "### Using the gini coefficient\n",
        "We talked about the gini index in the videos.  The Gini coefficient for a **given split** is given by:\n",
        "$$Gini=\\sum_{t=1}^T \\frac{n_t}{N}gini(t)$$\n",
        "where $T$ is the total number of splits (2 for binary attributes), $n_t$ is the number of instances in node $t$ after splitting, and $N$ is the total number of instances in the parent node. $gini(t)$ is the **gini index for each individual node that is created by the split** and is given by:\n",
        "$$gini(t)=1-\\sum_{j=0}^{C-1} p(j|t)^2$$\n",
        "where $C$ is the total number of possible classes and $p(j|t)$ is the probability of class $j$ in node $t$ (i.e., $n_j==$ the count of instances belonging to class $j$ in node $t$, normalized by the total number of instances in node $t$).\n",
        "$$ p(j|t) = \\frac{n_j}{n_t}$$\n",
        "\n",
        "For the given dataset, $gini(t)$ has been programmed for you in the function `gini_index`.\n",
        "\n",
        "* `def gini_index(classes_in_split):`\n",
        " * To use the function, pass in a `numpy` array of the class labels for a node as (i.e., pass in the rows from `ds.target` that make up a node in the tree) and the gini will be returned for that node.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G0jlPAQGZ1bt"
      },
      "outputs": [],
      "source": [
        "# compute the gini of several examples for the starting dataset\n",
        "# This function \"gini_index\" is written for you. Once you run this block, you\n",
        "#   will have access to the function for the notebook. You do not need to know\n",
        "#   how this function works--only what it returns\n",
        "# This function returns the gini index for an array of classes in a node.\n",
        "def gini_index(classes_in_split):\n",
        "    # pay no attention to this code in the function-- it just computes the gini for a given split\n",
        "    classes_in_split = np.reshape(classes_in_split,(len(classes_in_split),-1))\n",
        "    unique_classes = np.unique(classes_in_split)\n",
        "    gini = 1\n",
        "    for c in unique_classes:\n",
        "        gini -= (np.sum(classes_in_split==c) / float(len(classes_in_split)))**2\n",
        "\n",
        "    return gini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y933-Ru5Z1bu"
      },
      "source": [
        "In the example below, the function is used calculate the gini for splitting the dataset on feature 28, with value 2.5. In this example, we need to create two separate tree nodes: the first node has all the `ds.target` labels when feature 28 is greater than 2.5, the second node has all the rows from `ds.target` where feature 28 is less than 2.5. The steps are outlined below. **Read this carefully to understand what the code does below in the block following this.**\n",
        "- Feature 28 is saved into a separate variable `feature28 = ds.data[:,28]`\n",
        "- First all the target classes for the first node are calculated using `numpy` indexing `ds.target[feature28>2.5]`\n",
        " - Note: this grabs all the rows in `ds.target` (the classes) which have feature 28 greater than 2.5 (similar to indexing in pandas)\n",
        "- Second, those classes are passed into the function to get the gini for the right node in this split (i.e., feature 28 being greater than the threshold 2.5).\n",
        " - `gini_r = gini_index(ds.target[feature28>2.5])`\n",
        "- Third, the gini is calculated for the left node in the tree. This grabs only the rows in `ds.target` where feature 28 is less than 2.5.\n",
        "     - `gini_l = gini_index(ds.target[feature28<=2.5])`\n",
        "- Combining the gini indices is left as an exercise in the next section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j074fOWUZ1bu",
        "outputId": "ebf2c09f-9012-45f8-c9b5-feb17005151a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gini for right node of split: 0.8845857867667073\n",
            "gini for left node of split: 0.7115407566535388\n"
          ]
        }
      ],
      "source": [
        "#==========================Use the gini_index Example===============\n",
        "# get the value for this feature as a column vector\n",
        "# (this is like grabbing one column of the record table)\n",
        "feature28 = ds.data[:,28]\n",
        "\n",
        "# if we split on the value of 2.5, then this is the gini for each resulting node:\n",
        "gini_r = gini_index(ds.target[feature28>2.5]) # just like in pandas, we are sending in the rows where feature28>2.5\n",
        "gini_l = gini_index(ds.target[feature28<=2.5]) # and sending the rows where feature28<=2.5\n",
        "\n",
        "# compute gini example. This splits on attribute '28' with a value of 2.5\n",
        "print('gini for right node of split:', gini_r)\n",
        "print('gini for left node of split:', gini_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDWbJIJjZ1bu"
      },
      "source": [
        "Now, using the above values `gini_r` and `gini_l`. Calculate the combined Gini for the entire split. You will need to write the weighted summation (based upon the number of instances inside each node). To count the number of instances greater than a value using numpy, you can use broadcasting, which is a special way of indexing into a numpy array. For example, the code `some_array>5` will return a new numpy array of true/false elements. It is the same size as `some_array` and is marked true where the array is greater than `5`, and false otherwise. By taking the `sum` of this array, we can count how many times `some_array` is greater than `5`.\n",
        "\n",
        "`counts = sum(some_array>5)`\n",
        "\n",
        "You will need to use this syntax to count the values in each node as a result of splitting.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D7ldmOV2Z1bu",
        "outputId": "36884acb-54c1-46f9-eb8a-ccc35eda66c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1398.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "feature28 = ds.data[:,28]\n",
        "float(sum(feature28>2.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fl2O1zV9Z1bv",
        "outputId": "dbd14a93-02bb-4f80-a279-c0a4b8a27013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total gini of the split for a threshold of 2.5 is: 0.8461634345045179\n"
          ]
        }
      ],
      "source": [
        "# we need to make a weighted sum of the gini indices\n",
        "num_instances_r = float(sum(feature28>2.5))\n",
        "num_instances_l = float(sum(feature28<=2.5))\n",
        "N = float(len(ds.target))\n",
        "\n",
        "gini_total = (num_instances_r*gini_r + num_instances_l*gini_l) / N\n",
        "print('The total gini of the split for a threshold of 2.5 is:',gini_total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2C8HEdpZ1bv"
      },
      "source": [
        "**Question 2:** Now we want to know which of the two is a better split:\n",
        "- `feature28` split on a value of `2.5`  \n",
        "- `feature28` split on a value of `10`\n",
        "\n",
        "Enter your code to find the total gini of splitting on the threshold of 10 and compare it to the total gini of splitting on threshold of 2.5 (for feature 28 only).\n",
        "\n",
        "According to Gini, which threshold is better for spliting on feature 28, `threshold=2.5` or `threshold=10.0`? Explain.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The threshold of 10 is not better than the threshold of 2.5 because its greater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gqkOXGxEZ1bv",
        "outputId": "cd21d8a4-080b-4f6c-a955-d7a1be2fb78f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gini for right node of split: 0.8737186870604284\n",
            "gini for left node of split: 0.8496295618768864\n",
            "The total gini of the split for a threshold of 10 is: 0.8636111743234274\n",
            "The threshold of 10 is not better than the threshold of 2.5\n"
          ]
        }
      ],
      "source": [
        "# Enter your code here\n",
        "\n",
        "# if we split on the value of 10, then this is the gini for each resulting node:\n",
        "gini_r10 = gini_index(ds.target[feature28>10.0]) # just like in pandas, we are sending in the rows where feature28>2.5\n",
        "gini_l10 = gini_index(ds.target[feature28<=10.0]) # and sending the rows where feature28<=10.0\n",
        "\n",
        "# compute gini example. This splits on attribute '28' with a value of 1.0\n",
        "print('gini for right node of split:', gini_r10)\n",
        "print('gini for left node of split:', gini_l10)\n",
        "\n",
        "feature28 = ds.data[:,28]\n",
        "float(sum(feature28>10.0))\n",
        "num_instances_r10 = float(sum(feature28>10.0))\n",
        "num_instances_l10 = float(sum(feature28<=10.0))\n",
        "N = float(len(ds.target\n",
        "               ))\n",
        "\n",
        "gini_total10 = (num_instances_r10*gini_r10 + num_instances_l10*gini_l10) / N\n",
        "print('The total gini of the split for a threshold of 10 is:',gini_total10)\n",
        "\n",
        "# Enter your code here\n",
        "print('The threshold of 10 is not better than the threshold of 2.5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o29EI5qxZ1bv"
      },
      "source": [
        "### Entropy based splitting\n",
        "We discussed entropy as well in the video as another means of splitting. We calculated entropy for a node $t$ by:\n",
        "$$ Entropy(t) = -\\sum p(j|t) \\log p(j|t) $$\n",
        "where $p(j|t)$ is the same as above. To combine Entropy measures from a set of nodes, t = {1,...,T} we use:\n",
        "$$Entropy_{split}=\\sum_{t=1}^T \\frac{n_t}{N}Entropy(t)$$\n",
        "where $n_t$ and $N$ are the same as defined above for the $Gini$. Information gain is calculated by subtracting the Entropy of the split from the Entropy of the parent node before splitting:\n",
        "$$InfoGain = Entropy(p)-Entropy_{split}$$\n",
        "where $p$ is the parent node before splitting. You are given an equation for calculating the $Entropy(t)$ of  node $t$. It works exactly like the `gini_index` function above, but is named `entropy_value` and returns the entropy for a node. You simply send in an array of the feature values for the node you want to calculate the entropy value for.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h3pIHjBmZ1bw"
      },
      "outputs": [],
      "source": [
        "def entropy_value(classes_in_split):\n",
        "    # pay no attention to this code -- it just computes the gini for a given split\n",
        "    classes_in_split = np.reshape(classes_in_split,(len(classes_in_split),-1))\n",
        "    unique_classes = np.unique(classes_in_split)\n",
        "    ent = 0\n",
        "    for c in unique_classes:\n",
        "        p = (np.sum(classes_in_split==c) / float(len(classes_in_split)))\n",
        "        ent += p * np.log(p)\n",
        "\n",
        "    return -ent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kbfIj-PJZ1bw",
        "outputId": "b2c31e0b-cca4-4114-a8b4-4fb78bd03fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entropy for right node of split: 2.1836975378213057\n",
            "entropy for left node of split: 1.4898881412786364\n"
          ]
        }
      ],
      "source": [
        "ent_r = entropy_value(ds.target[feature28>2.5])\n",
        "ent_l = entropy_value(ds.target[feature28<=2.5])\n",
        "\n",
        "# compute entropy example. This splits on attribute '28' with a value of 2.5\n",
        "print('entropy for right node of split:', ent_r)\n",
        "print('entropy for left node of split:', ent_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QikDclvZ1bw"
      },
      "source": [
        "**Question 3:** Calculate the **information gain** of the split when the threshold is 2.5 on `feature28`. What is the value of the information gain?\n",
        "The information gain of the split for threshold of 2.5 is 0.27283285132716273"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qKIN6f3wZ1bw",
        "outputId": "5bce72d0-ebcb-4622-81ea-2eafdbc016c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of parent node: 2.302479220967876\n",
            "Entropy for right node of split: 2.1836975378213057\n",
            "Entropy for left node of split: 1.4898881412786364\n",
            "Total entropy after split: 2.029646369640713\n",
            "Information gain of the split: 0.27283285132716273\n",
            "The information gain of the split for threshold of 2.5: 0.27283285132716273\n"
          ]
        }
      ],
      "source": [
        "# Enter your code here\n",
        "# Calculate the entropy for the parent node\n",
        "ent_parent = entropy_value(ds.target)\n",
        "\n",
        "\n",
        "# Number of instances in the right and left nodes\n",
        "num_instances_r = float(sum(feature28 > 2.5))\n",
        "num_instances_l = float(sum(feature28 <= 2.5))\n",
        "N = float(len(ds.target))\n",
        "\n",
        "# Calculate the total entropy after the split\n",
        "ent_total_split = (num_instances_r * ent_r + num_instances_l * ent_l) / N\n",
        "\n",
        "# Calculate the information gain\n",
        "info_gain = ent_parent - ent_total_split\n",
        "\n",
        "print(\"Entropy of parent node:\", ent_parent)\n",
        "print(\"Entropy for right node of split:\", ent_r)\n",
        "print(\"Entropy for left node of split:\", ent_l)\n",
        "print(\"Total entropy after split:\", ent_total_split)\n",
        "print(\"Information gain of the split:\", info_gain)\n",
        "\n",
        "\n",
        "\n",
        "# Enter your code here\n",
        "print('The information gain of the split for threshold of 2.5:', info_gain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew-JL7JkZ1bw"
      },
      "source": [
        "**Question 4:** What is the information gain if the threshold is 10.0 on `feature28`? According to information gain, is it better to split on a threshold of 2.5 or 10? Does entropy give the same decision as gini for this example?\n",
        "\n",
        "**Answer:** The information gain of the split for threshold of 10: 0.20955137704371163. This is not better than the split on 2.5. The Gini index also suggested that threshold 2.5 was a better split. Thus, entropy gives the same decision as Gini for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZJOEoKw_Z1bx",
        "outputId": "714f6d5c-938c-464d-cce9-0ca10ca38969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of parent node: 2.302479220967876\n",
            "Entropy for right node of split (threshold 10): 2.112391791714538\n",
            "Entropy for left node of split (threshold 10): 2.066003576622626\n",
            "Total entropy after split (threshold 10): 2.0929278439241643\n",
            "The information gain of the split for threshold of 10: 0.20955137704371163\n",
            "This is not better than the split on 2.5\n",
            "This is not the same as gini\n",
            "This (is/is not) the same as gini\n"
          ]
        }
      ],
      "source": [
        "# Enter your code here\n",
        "#Calculate the entropies for the right and left nodes of the split on feature 28 with threshold 10\n",
        "ent_r_10 = entropy_value(ds.target[feature28 > 10])\n",
        "ent_l_10 = entropy_value(ds.target[feature28 <= 10])\n",
        "\n",
        "# Number of instances in the right and left nodes\n",
        "num_instances_r_10 = float(sum(feature28 > 10))\n",
        "num_instances_l_10 = float(sum(feature28 <= 10))\n",
        "\n",
        "# Calculate the total entropy after the split\n",
        "ent_total_split_10 = (num_instances_r_10 * ent_r_10 + num_instances_l_10 * ent_l_10) / N\n",
        "\n",
        "# Calculate the information gain\n",
        "info_gain_10 = ent_parent - ent_total_split_10\n",
        "\n",
        "print(\"Entropy of parent node:\", ent_parent)\n",
        "print(\"Entropy for right node of split (threshold 10):\", ent_r_10)\n",
        "print(\"Entropy for left node of split (threshold 10):\", ent_l_10)\n",
        "print(\"Total entropy after split (threshold 10):\", ent_total_split_10)\n",
        "\n",
        "\n",
        "\n",
        "# Enter your code here\n",
        "\n",
        "print('The information gain of the split for threshold of 10:', info_gain_10)\n",
        "print('This is better than the split on 2.5') if info_gain_10 > info_gain else print('This is not better than the split on 2.5')\n",
        "print('This is the same as gini') if (info_gain_10 > info_gain and gini_total10 > gini_total) or (info_gain_10 < info_gain and gini_total10 < gini_total) else print('This is not the same as gini')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gF8mlnJZ1bx"
      },
      "source": [
        "### Information gain and multi-way splitting\n",
        "Now assume that we can use not just a binary split, but a three way split.\n",
        "\n",
        "**Question 5**: What is the information gain if we split feature28 on two thesholds (three separate nodes corresponding to three branches from one node)\n",
        "- node left: `feature28<2.5`,\n",
        "- node middle: `2.5<=feature28<10`, and\n",
        "- node right: `10<=feature28`?\n",
        "\n",
        "Is the information gain better?\n",
        "\n",
        "**Answer:** The information gain for the three-way split (0.3171890999123379) is higher than the information gain for both binary splits at thresholds 2.5 (0.27283285132716273) and 10 (0.20955137104371163).\n",
        "\n",
        "Therefore, the three-way split on feature 28 with thresholds at 2.5 and 10 provides better information gain compared to the binary splits. This indicates that the three-way split is more effective in reducing entropy and improving the classification performance in this case.\n",
        "\n",
        "\n",
        "***Note***: You can index into a `numpy` array for the middle node with the following notation: `some_array[(2.5<=feature28) & (feature28<10.0)]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eXWcAOmdZ1bx",
        "outputId": "15fc52db-9040-42e5-8768-8f0a1571e6bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of parent node: 2.302479220967876\n",
            "Entropy for left node of split: 1.4898881412786364\n",
            "Entropy for middle node of split: 2.1558341564612853\n",
            "Entropy for right node of split: 2.118750287884169\n",
            "Total entropy after three-way split: 1.985290121055538\n",
            "Information gain of the three-way split: 0.3171890999123379\n"
          ]
        }
      ],
      "source": [
        "# Enter your code here\n",
        "# Calculate the entropies for the left, middle, and right nodes of the three-way split on feature 28 with thresholds 2.5 and 10\n",
        "ent_left = entropy_value(ds.target[feature28 < 2.5])\n",
        "ent_middle = entropy_value(ds.target[(2.5 <= feature28) & (feature28 < 10)])\n",
        "ent_right = entropy_value(ds.target[feature28 >= 10])\n",
        "\n",
        "# Number of instances in the left, middle, and right nodes\n",
        "num_instances_left = float(sum(feature28 < 2.5))\n",
        "num_instances_middle = float(sum((2.5 <= feature28) & (feature28 < 10)))\n",
        "num_instances_right = float(sum(feature28 >= 10))\n",
        "\n",
        "# Calculate the total entropy after the three-way split\n",
        "ent_total_split_three_way = (\n",
        "    (num_instances_left * ent_left +\n",
        "     num_instances_middle * ent_middle +\n",
        "     num_instances_right * ent_right) / N\n",
        ")\n",
        "\n",
        "# Calculate the information gain for the three-way split\n",
        "info_gain_three_way = ent_parent - ent_total_split_three_way\n",
        "\n",
        "\n",
        "\n",
        "# Enter your code here\n",
        "print(\"Entropy of parent node:\", ent_parent)\n",
        "print(\"Entropy for left node of split:\", ent_left)\n",
        "print(\"Entropy for middle node of split:\", ent_middle)\n",
        "print(\"Entropy for right node of split:\", ent_right)\n",
        "print(\"Total entropy after three-way split:\", ent_total_split_three_way)\n",
        "print(\"Information gain of the three-way split:\", info_gain_three_way)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh8DCXM8Z1bx"
      },
      "source": [
        "**Question 6**: Should we normalize the quantity that we just calculated if we want to compare it to the information gain of a binary split? Why or why not? Explain.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hh1_P-1YZ1bx"
      },
      "outputs": [],
      "source": [
        "# Enter your comments here\n",
        "Normalization would be required if the nature of the splits or the quantity being compared involved different scales or distributions that might otherwise distort the comparison.\n",
        "Normalization of the information gain is not necessary when comparing binary and multi-way splits on the same dataset. The information gain metric inherently adjusts for the distribution of instances among the child nodes, allowing for a fair comparison of the effectiveness of different splitting strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Enter your comments here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwfPxlFEZ1bx"
      },
      "source": [
        "### Decision Trees in scikit-learn\n",
        "Scikit-learn also has an implementation of decision trees. Its available here:\n",
        "- http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
        "\n",
        "**Question 7**: Read the documention. What algorithm does scikit-learn use for creating decision trees (i.e., ID3, C4.5, C5.0, CART, MARS, CHAID, etc.)?\n",
        "\n",
        "**Answer:** scikit-learn uses the Classification and Regression Trees (CART) algorithm. The CART algorithm constructs binary trees using the feature and threshold that yield the largest information gain at each node​"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zqryL3fZ1by"
      },
      "source": [
        "**Question 8**: Using the documentation, use scikit-learn to train a decision tree on the digits data. Calculate the accuracy on the training data. What is the accuracy? Did you expect the decision tree to have this kind of accuracy? Why or why not? Explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iE0JullIZ1by",
        "outputId": "26382f2e-1818-4c2f-d58a-116f90f3528b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 1.0\n",
            "I did expect this kind of accuracy because decision trees tend to overfit on the training data, resulting in very high accuracy on the training set. However, this does not necessarily reflect the performance on unseen data.}\n"
          ]
        }
      ],
      "source": [
        "# use scikit learn to train a decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# enter your code below here to train and predict on the same data\n",
        "\n",
        "# Load the dataset\n",
        "digits = load_digits()\n",
        "X, y = ds.data, ds.target\n",
        "\n",
        "# Train the decision tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Predict on the same data\n",
        "y_pred = clf.predict(X)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "\n",
        "# enter your code above here\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# enter your code below here to calculate accuracy\n",
        "# Print the accuracy\n",
        "print('accuracy:', accuracy)\n",
        "\n",
        "print('I did expect this kind of accuracy because decision trees tend to overfit on the training data, resulting in very high accuracy on the training set. However, this does not necessarily reflect the performance on unseen data.}')\n",
        "# enter your code above here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7A7SeNjZ1by"
      },
      "source": [
        "**Question 9**: Look at the other input parameters to the function `DecisionTreeClassifier`-- could any of them be used to help prevent the decision tree from overlearning on the data?\n",
        "\n",
        "Which variables might we use to control overfitting and how (explain why it might help to stop overfitting)?\n",
        "\n",
        "**Answer:** We could use max_features: The number of features to consider when looking for the best split. If not specified, all features are considered.Using a subset of features can reduce the model's complexity and help prevent overfitting, especially if the data has many features.\n",
        "\n",
        "Using max_leaf_nodes limits the number of leaf nodes, simplifying the tree.\n",
        "\n",
        "Using these parameters helps to control the complexity of the decision tree, thereby reducing the risk of overfitting and improving the model's generalization to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYYqACRpZ1by"
      },
      "source": [
        "________________________________________________________________________________________________________\n",
        "\n",
        "That's all! Please **upload your rendered notebook** (.ipynb file!).\n",
        "Also please remember to save the notebook before uploading.\n",
        "\n",
        "\n",
        "\n",
        "Grading Schema:\n",
        "\n",
        "Q1: 5;\n",
        "\n",
        "Q2: 15;\n",
        "\n",
        "Q3: 10;\n",
        "\n",
        "Q4: 10;\n",
        "\n",
        "Q5: 10;\n",
        "\n",
        "Q6: 15\n",
        "\n",
        "Q7: 10\n",
        "\n",
        "Q8: 15\n",
        "\n",
        "Q9: 10"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}